{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b814edb2-30b8-4d73-805e-e3db70535175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import njit, set_num_threads, prange\n",
    "\n",
    "import math\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b99ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 输入数据：one-hot encode为-1和1\n",
    "@njit(nogil=True)\n",
    "def compute_rank_numba(values, weights, epsilon):\n",
    "    total_weight = np.sum(weights)\n",
    "    target = epsilon * total_weight\n",
    "    current_weight = 0.0\n",
    "    candidates = []\n",
    "    sorted_idx = np.argsort(values)\n",
    "    sorted_values = values[sorted_idx]\n",
    "    sorted_weights = weights[sorted_idx]\n",
    "    \n",
    "    for i in prange(len(sorted_values)):\n",
    "        current_weight += sorted_weights[i]\n",
    "        if current_weight >= target:\n",
    "            candidates.append(sorted_values[i])\n",
    "            current_weight = 0.0\n",
    "    return np.array(candidates)\n",
    "    \n",
    "#---------------------------------#\n",
    "# exact split\n",
    "@njit(nogil=True)\n",
    "def compute_split_gain(g_left, h_left, g_total, h_total, reg_lambda, gamma, min_child_weight):\n",
    "    \"\"\"计算单个分割点的增益\"\"\"\n",
    "    if (h_left < min_child_weight) or ((h_total - h_left) < min_child_weight):\n",
    "        return 0.0\n",
    "    left_gain = g_left * g_left / (h_left + reg_lambda)\n",
    "    right_gain = (g_total - g_left) * (g_total - g_left) / (h_total - h_left + reg_lambda)\n",
    "    root_gain = g_total * g_total / (h_total + reg_lambda)\n",
    "    return 0.5 * (left_gain + right_gain - root_gain) - gamma\n",
    "\n",
    "@njit(nogil=True)\n",
    "def find_split_exact_single_feature(values, g_sorted, h_sorted, \n",
    "                                  total_g, total_h, reg_lambda, gamma, min_child_weight):\n",
    "    \"\"\"计算单个特征的最佳分割点\"\"\"\n",
    "    g_cum = np.cumsum(g_sorted)\n",
    "    h_cum = np.cumsum(h_sorted)\n",
    "    \n",
    "    best_gain = 0.0\n",
    "    best_threshold = 0.0\n",
    "    \n",
    "    for j in range(1, len(values)):\n",
    "        if values[j] == values[j-1]:\n",
    "            continue\n",
    "            \n",
    "        gain = compute_split_gain(g_cum[j-1], h_cum[j-1], \n",
    "                                total_g, total_h,\n",
    "                                reg_lambda, gamma, min_child_weight)\n",
    "        \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_threshold = (values[j-1] + values[j]) / 2\n",
    "    \n",
    "    return best_gain, best_threshold\n",
    "#-------------------------------------#\n",
    "# approximate split\n",
    "@njit(nogil=True)\n",
    "def find_split_approx_single_feature(values, local_indices, g, h, candidates,\n",
    "                                   total_g, total_h, reg_lambda, gamma, min_child_weight):\n",
    "    \"\"\"计算单个特征的最佳近似分割点\"\"\"\n",
    "    best_gain = 0.0\n",
    "    best_threshold = None\n",
    "    \n",
    "    for thresh in candidates:\n",
    "        left_mask = values <= thresh\n",
    "        sum_g_left = g[local_indices[left_mask]].sum()\n",
    "        sum_h_left = h[local_indices[left_mask]].sum()\n",
    "        sum_h_right = total_h - sum_h_left\n",
    "        \n",
    "        if sum_h_left < min_child_weight or sum_h_right < min_child_weight:\n",
    "            continue\n",
    "            \n",
    "        gain = compute_split_gain(sum_g_left, sum_h_left,\n",
    "                                total_g, total_h,\n",
    "                                reg_lambda, gamma, min_child_weight)\n",
    "        \n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_threshold = thresh\n",
    "    \n",
    "    return best_gain, best_threshold\n",
    "#-------------------------------------#\n",
    "# sparse split\n",
    "@njit(nogil=True)\n",
    "def _process_sparse_feature(values, g_non_miss, h_non_miss, sum_g_miss, sum_h_miss, \n",
    "                          total_g, total_h, reg_lambda, gamma, min_child_weight):\n",
    "    \"\"\"使用 numba 加速稀疏特征的预处理计算\"\"\"\n",
    "    g_cum = np.cumsum(g_non_miss)\n",
    "    h_cum = np.cumsum(h_non_miss)\n",
    "    return g_cum, h_cum\n",
    "    \n",
    "@njit(nogil=True)\n",
    "def find_split_sparse_exact_single_feature_fast(values, g_cum, h_cum,\n",
    "                                              sum_g_miss, sum_h_miss,\n",
    "                                              total_g, total_h, reg_lambda, gamma, min_child_weight):\n",
    "    \"\"\"优化后的稀疏特征精确分割算法\"\"\"\n",
    "    best_gain = 0.0\n",
    "    best_threshold = None\n",
    "    best_default_left = None\n",
    "    has_missing = (sum_g_miss != 0) or (sum_h_miss != 0)\n",
    "    \n",
    "    # 预分配数组以避免重复计算\n",
    "    gains_left = np.zeros(len(values), dtype=np.float32)\n",
    "    gains_right = np.zeros(len(values), dtype=np.float32)\n",
    "    \n",
    "    # 并行计算所有可能的分割点的增益\n",
    "    for i in prange(1, len(values)):\n",
    "        if values[i] == values[i-1]:\n",
    "            continue\n",
    "            \n",
    "        sum_g_left = g_cum[i-1]\n",
    "        sum_h_left = h_cum[i-1]\n",
    "        sum_h_right = total_h - sum_h_left - sum_h_miss\n",
    "        \n",
    "        if sum_h_left < min_child_weight or sum_h_right < min_child_weight:\n",
    "            continue\n",
    "        \n",
    "        if has_missing:\n",
    "            gains_left[i] = compute_split_gain(\n",
    "                sum_g_left + sum_g_miss, sum_h_left + sum_h_miss,\n",
    "                total_g, total_h, reg_lambda, gamma, min_child_weight\n",
    "            )\n",
    "            gains_right[i] = compute_split_gain(\n",
    "                sum_g_left, sum_h_left,\n",
    "                total_g, total_h, reg_lambda, gamma, min_child_weight\n",
    "            )\n",
    "            current_gain = max(gains_left[i], gains_right[i])\n",
    "            current_default_left = gains_left[i] > gains_right[i]\n",
    "        else:\n",
    "            current_gain = compute_split_gain(\n",
    "                sum_g_left, sum_h_left,\n",
    "                total_g, total_h, reg_lambda, gamma, min_child_weight\n",
    "            )\n",
    "            current_default_left = True\n",
    "        \n",
    "        if current_gain > best_gain:\n",
    "            best_gain = current_gain\n",
    "            best_threshold = (values[i-1] + values[i]) / 2\n",
    "            best_default_left = current_default_left if has_missing else True\n",
    "            \n",
    "    return best_gain, best_threshold, best_default_left\n",
    "\n",
    "@njit(nogil=True)\n",
    "def find_split_sparse_approx_single_feature_fast(values, g_cum, h_cum,\n",
    "                                               sum_g_miss, sum_h_miss, candidates,\n",
    "                                               total_g, total_h, reg_lambda, gamma, min_child_weight):\n",
    "    \"\"\"优化后的稀疏特征近似分割算法\"\"\"\n",
    "    best_gain = 0.0\n",
    "    best_threshold = None\n",
    "    best_default_left = None\n",
    "    has_missing = (sum_g_miss != 0) or (sum_h_miss != 0)\n",
    "    \n",
    "    # 预分配数组以避免重复计算\n",
    "    n_candidates = len(candidates)\n",
    "    gains_left = np.zeros(n_candidates, dtype=np.float32)\n",
    "    gains_right = np.zeros(n_candidates, dtype=np.float32)\n",
    "    \n",
    "    # 预计算每个候选点的左侧累积和\n",
    "    for i in prange(n_candidates):\n",
    "        thresh = candidates[i]\n",
    "        left_idx = np.searchsorted(values, thresh, side='right') - 1\n",
    "        \n",
    "        if left_idx < 0:\n",
    "            continue\n",
    "            \n",
    "        sum_g_left = g_cum[left_idx]\n",
    "        sum_h_left = h_cum[left_idx]\n",
    "        sum_h_right = total_h - sum_h_left - sum_h_miss\n",
    "        \n",
    "        if sum_h_left < min_child_weight or sum_h_right < min_child_weight:\n",
    "            continue\n",
    "        \n",
    "        if has_missing:\n",
    "            # 计算将缺失值分到左边的增益\n",
    "            gains_left[i] = compute_split_gain(\n",
    "                sum_g_left + sum_g_miss, sum_h_left + sum_h_miss,\n",
    "                total_g, total_h, reg_lambda, gamma, min_child_weight\n",
    "            )\n",
    "            # 计算将缺失值分到右边的增益\n",
    "            gains_right[i] = compute_split_gain(\n",
    "                sum_g_left, sum_h_left,\n",
    "                total_g, total_h, reg_lambda, gamma, min_child_weight\n",
    "            )\n",
    "            current_gain = max(gains_left[i], gains_right[i])\n",
    "            current_default_left = gains_left[i] > gains_right[i]\n",
    "        else:\n",
    "            current_gain = compute_split_gain(\n",
    "                sum_g_left, sum_h_left,\n",
    "                total_g, total_h, reg_lambda, gamma, min_child_weight\n",
    "            )\n",
    "            current_default_left = True\n",
    "        \n",
    "        if current_gain > best_gain:\n",
    "            best_gain = current_gain\n",
    "            best_threshold = thresh\n",
    "            best_default_left = current_default_left if has_missing else True\n",
    "            \n",
    "    return best_gain, best_threshold, best_default_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef3a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分位图草图\n",
    "class WeightedQuantileSketch:\n",
    "    def __init__(self, eps=0.05, max_size=100):\n",
    "        self.eps = eps\n",
    "        self.max_size = max_size\n",
    "        self.summaries = defaultdict(list)\n",
    "    \n",
    "    def _compute_rank(self, values, weights, epsilon):\n",
    "        return compute_rank_numba(values, weights, epsilon)\n",
    "    \n",
    "    def create_summary(self, feature_idx, values, hessians):\n",
    "        candidates = self._compute_rank(values, hessians, self.eps)\n",
    "        self.summaries[feature_idx] = candidates\n",
    "    \n",
    "    def get_candidates(self, feature_idx):\n",
    "        return self.summaries.get(feature_idx, [])\n",
    "\n",
    "# 压缩列存储块结构CSC\n",
    "class ColumnBlock:\n",
    "    def __init__(self, data, feature_idx, sample_indices):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.sample_indices = sample_indices  # 当前节点的样本索引（指向原始数据）\n",
    "        self.data = data[self.sample_indices, feature_idx].astype(np.float32)\n",
    "\n",
    "        self.sorted_idx = np.argsort(self.data)  # 对当前子集排序的局部索引（0到len(sample_indices)-1）\n",
    "        self.sorted_values = self.data[self.sorted_idx]\n",
    "\n",
    "        # one-hot encode 用-1表示缺失值，避免0与连续变量的值混淆\n",
    "        self.missing_mask = np.isnan(self.sorted_values) | (self.sorted_values == -1.0)\n",
    "\n",
    "    def get_subset(self, sub_indices,exact=True):\n",
    "        # 返回sorted_values和sorted sample_indices的子集\n",
    "        sub_mask = np.isin(self.sample_indices[self.sorted_idx], sub_indices)\n",
    "        if exact:\n",
    "            return self.sorted_values[sub_mask], self.sample_indices[self.sorted_idx[sub_mask]]\n",
    "        else:\n",
    "            return self.sorted_values[sub_mask], self.sorted_idx[sub_mask]\n",
    "            \n",
    "    def get_missing(self, sub_indices):\n",
    "        sub_mask = np.isin(self.sample_indices[self.sorted_idx], sub_indices)\n",
    "        valid_mask = (~self.missing_mask) & sub_mask\n",
    "        missing_mask = self.missing_mask & sub_mask\n",
    "        return (self.sorted_values[valid_mask], \n",
    "                self.sample_indices[self.sorted_idx[valid_mask]],\n",
    "                self.sample_indices[self.sorted_idx[missing_mask]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcadea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BoostTreeNode:\n",
    "    def __init__(self, feature_idx=None, threshold=None, left=None, right=None, value=None, default_left=None):\n",
    "        self.feature_idx = feature_idx\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.default_left = default_left\n",
    "        self.best_gain = 0.0\n",
    "\n",
    "class BoosterTree:\n",
    "    def __init__(self, params, n_threads=1): \n",
    "        self.root = None\n",
    "\n",
    "        self.max_depth = params.get('max_depth', 5)\n",
    "        self.reg_lambda = params.get('reg_lambda', 1.0)\n",
    "        self.gamma = params.get('gamma', 0.0)\n",
    "        self.min_child_weight = params.get('min_child_weight', 1.0)\n",
    "        self.colsample_bynode = params.get('colsample_bynode', 1.0)\n",
    "        self.method = params.get('method', 'exact')\n",
    "        self.sparse = params.get('sparse', False)\n",
    "\n",
    "        self.column_blocks = None  # 每棵树使用column blocks来并行\n",
    "        self.X = None \n",
    "        self.grad_stats = None  \n",
    "        self.quantile_sketch = WeightedQuantileSketch(eps=0.05) if self.method == 'approx' else None\n",
    "\n",
    "        self.n_threads = n_threads\n",
    "\n",
    "    def fit(self, X, grad_stats, indices=None):\n",
    "        if indices is None:\n",
    "            indices = np.arange(len(grad_stats))\n",
    "        \n",
    "        self.X = X\n",
    "        self.grad_stats = grad_stats\n",
    "        self.n_samples, self.n_features = len(indices), X.shape[1]\n",
    "        \n",
    "        # 初始化列块存储\n",
    "        self.column_blocks = [ColumnBlock(self.X, fidx, indices) for fidx in range(self.n_features)]\n",
    "\n",
    "        # 近似分位算法\n",
    "        if self.method == 'approx':\n",
    "            self._prepare_quantile_sketches(indices)\n",
    "    \n",
    "        self.root = self._grow_tree(indices, depth=0)\n",
    "    \n",
    "    def _prepare_quantile_sketches(self, indices):\n",
    "        \"\"\"并行处理特征的分位数统计\"\"\"\n",
    "        self.non_missing_masks = []\n",
    "        X_subset = self.X[indices]\n",
    "        h = self.grad_stats[indices, 1]\n",
    "\n",
    "        def process_feature(fidx):\n",
    "            try:\n",
    "                feature_values = X_subset[:, fidx]\n",
    "                non_missing_mask = ~np.isnan(feature_values)\n",
    "                values = feature_values[non_missing_mask]\n",
    "                h_non_missing = h[non_missing_mask]\n",
    "                return fidx, non_missing_mask, values, h_non_missing\n",
    "            except Exception as e:\n",
    "                print(f\"处理特征 {fidx} 时出错: {e}\")\n",
    "                return fidx, None, None, None\n",
    "\n",
    "        # 使用线程池并行处理特征\n",
    "        with ThreadPoolExecutor(max_workers=self.n_threads) as executor:\n",
    "            futures = []\n",
    "            for fidx in range(self.n_features):\n",
    "                futures.append(executor.submit(process_feature, fidx))\n",
    "            \n",
    "            # 收集结果并创建摘要\n",
    "            self.non_missing_masks = [None] * self.n_features\n",
    "            for future in futures:\n",
    "                fidx, mask, values, h_values = future.result()\n",
    "                if mask is not None:\n",
    "                    self.non_missing_masks[fidx] = mask\n",
    "                    if len(values) > 0:\n",
    "                        self.quantile_sketch.create_summary(fidx, values, h_values)\n",
    "\n",
    "    def _grow_tree(self, sample_indices, depth=0):\n",
    "        g = self.grad_stats[sample_indices, 0]\n",
    "        h = self.grad_stats[sample_indices, 1]\n",
    "        node_value = -g.sum() / (h.sum() + self.reg_lambda)\n",
    "        node = BoostTreeNode(value=node_value)\n",
    "        \n",
    "        if depth >= self.max_depth or len(sample_indices) < 2:\n",
    "            return node\n",
    "    \n",
    "        # 随机特征子集\n",
    "        feature_indices = self._get_feature_subset()\n",
    "        \n",
    "        if self.method == 'exact' and not self.sparse:\n",
    "            best_gain, best_feature, best_threshold = self._find_split_exact(sample_indices, g, h, feature_indices)\n",
    "            if best_gain <= 0.0:\n",
    "                return node\n",
    "            node.feature_idx, node.threshold, node.best_gain = best_feature, best_threshold, best_gain\n",
    "            node.default_left = False\n",
    "            feature_values = self.X[sample_indices, best_feature]\n",
    "            left_mask = feature_values <= best_threshold\n",
    "            right_mask = ~left_mask\n",
    "\n",
    "        elif self.method == 'approx' and not self.sparse:\n",
    "            best_gain, best_feature, best_threshold = self._find_split_approx(sample_indices, g, h, feature_indices)\n",
    "            if best_gain <= 0.0:\n",
    "                return node\n",
    "            node.feature_idx, node.threshold, node.best_gain = best_feature, best_threshold, best_gain\n",
    "            node.default_left = False\n",
    "            feature_values = self.X[sample_indices, best_feature]\n",
    "            left_mask = feature_values <= best_threshold\n",
    "            right_mask = ~left_mask\n",
    "        \n",
    "        else:  # Sparse-aware (exact or approx)\n",
    "            best_gain, best_feature, best_threshold, best_default_left = self._find_split_sparse(\n",
    "                sample_indices, g, h, feature_indices, exact=(self.method == 'exact')\n",
    "            )\n",
    "            if best_gain <= 0.0:\n",
    "                return node\n",
    "            node.feature_idx, node.threshold, node.best_gain = best_feature, best_threshold, best_gain\n",
    "            node.default_left = best_default_left\n",
    "            feature_values = self.X[sample_indices, best_feature]\n",
    "            miss_mask = np.isnan(feature_values) | (feature_values == -1.0)\n",
    "            left_mask = ((feature_values <= best_threshold) & ~miss_mask) | (miss_mask & best_default_left)\n",
    "            right_mask = ~left_mask\n",
    "        \n",
    "        node.left = self._grow_tree(sample_indices[left_mask], depth + 1)\n",
    "        node.right = self._grow_tree(sample_indices[right_mask], depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _get_feature_subset(self):\n",
    "        n_selected = int(self.n_features * self.colsample_bynode)\n",
    "        return np.random.choice(self.n_features, n_selected, replace=False)\n",
    "\n",
    "    def _find_split_exact(self, sample_indices, g, h, feature_indices):\n",
    "        \"\"\"使用多线程并行计算最佳分割点\"\"\"\n",
    "        total_g, total_h = g.sum(), h.sum()\n",
    "        def process_feature(fidx):\n",
    "            block = self.column_blocks[fidx]\n",
    "            values, local_indices = block.get_subset(sample_indices, exact=True)\n",
    "            g_sorted = self.grad_stats[local_indices,0]\n",
    "            h_sorted = self.grad_stats[local_indices,1]\n",
    "\n",
    "            gain, threshold = find_split_exact_single_feature(\n",
    "                values,\n",
    "                g_sorted, h_sorted,\n",
    "                total_g, total_h,\n",
    "                self.reg_lambda,\n",
    "                self.gamma,\n",
    "                self.min_child_weight\n",
    "            )\n",
    "            \n",
    "            return gain, fidx, threshold\n",
    "        \n",
    "        # 多线程处理\n",
    "        with ThreadPoolExecutor(self.n_threads) as executor:\n",
    "            results = list(executor.map(process_feature, feature_indices))\n",
    "        \n",
    "        # 找出最佳分割点\n",
    "        best_result = max(results, key=lambda x: x[0])\n",
    "        return best_result[0], int(best_result[1]), best_result[2]    \n",
    "\n",
    "    def _find_split_approx(self, sample_indices, g, h, feature_indices):\n",
    "        \"\"\"使用多线程并行计算最佳近似分割点\"\"\"\n",
    "        total_g, total_h = g.sum(), h.sum()\n",
    "        \n",
    "        def process_feature(fidx):\n",
    "            block = self.column_blocks[fidx]\n",
    "            values, local_indices = block.get_subset(sample_indices, exact=False)\n",
    "            if len(values) == 0:\n",
    "                return 0.0, fidx, None\n",
    "\n",
    "            candidates = self.quantile_sketch.get_candidates(fidx)\n",
    "            if len(candidates) == 0:\n",
    "                return 0.0, fidx, None\n",
    "            \n",
    "            gain, threshold = find_split_approx_single_feature(\n",
    "                values, local_indices, g, h, candidates,\n",
    "                total_g, total_h,\n",
    "                self.reg_lambda, self.gamma, self.min_child_weight\n",
    "            )\n",
    "            \n",
    "            return gain, fidx, threshold\n",
    "        \n",
    "        # 多线程处理\n",
    "        with ThreadPoolExecutor(self.n_threads) as executor:\n",
    "            results = list(executor.map(process_feature, feature_indices))\n",
    "        \n",
    "        # 找出最佳分割点\n",
    "        best_result = max(results, key=lambda x: x[0])\n",
    "        return best_result[0], int(best_result[1]), best_result[2]\n",
    "\n",
    "    def _find_split_sparse(self, sample_indices, g, h, feature_indices, exact=True):\n",
    "        \"\"\"优化后的稀疏特征分割点查找\"\"\"\n",
    "        total_g, total_h = g.sum(), h.sum()\n",
    "    \n",
    "        def process_feature(fidx):\n",
    "            block = self.column_blocks[fidx]\n",
    "            values, non_missing_indices, missing_indices = block.get_missing(sample_indices)\n",
    "            if len(values) == 0:\n",
    "                return 0.0, fidx, None, None\n",
    "            \n",
    "            # 预计算梯度统计量\n",
    "            g_non_miss = self.grad_stats[non_missing_indices,0]\n",
    "            h_non_miss = self.grad_stats[non_missing_indices,1]\n",
    "            sum_g_miss = np.sum(self.grad_stats[missing_indices,0]) if len(missing_indices) > 0 else 0.0\n",
    "            sum_h_miss = np.sum(self.grad_stats[missing_indices,1]) if len(missing_indices) > 0 else 0.0\n",
    "            \n",
    "            # 使用 numba 加速的预处理\n",
    "            g_cum, h_cum = _process_sparse_feature(\n",
    "                values, g_non_miss, h_non_miss,\n",
    "                sum_g_miss, sum_h_miss,\n",
    "                total_g, total_h,\n",
    "                self.reg_lambda, self.gamma,\n",
    "                self.min_child_weight\n",
    "            )\n",
    "            \n",
    "            if exact:\n",
    "                gain, threshold, default_left = find_split_sparse_exact_single_feature_fast(\n",
    "                    values, g_cum, h_cum,\n",
    "                    sum_g_miss, sum_h_miss,\n",
    "                    total_g, total_h,\n",
    "                    self.reg_lambda, self.gamma, self.min_child_weight\n",
    "                )\n",
    "            else:\n",
    "                candidates = self.quantile_sketch.get_candidates(fidx)\n",
    "                gain, threshold, default_left = find_split_sparse_approx_single_feature_fast(\n",
    "                    values, g_cum, h_cum,\n",
    "                    sum_g_miss, sum_h_miss, candidates,\n",
    "                    total_g, total_h,\n",
    "                    self.reg_lambda, self.gamma, self.min_child_weight\n",
    "                )\n",
    "            \n",
    "            return gain, fidx, threshold, default_left\n",
    "\n",
    "        # 多线程处理\n",
    "        with ThreadPoolExecutor(self.n_threads) as executor:\n",
    "            results = list(executor.map(process_feature, feature_indices))\n",
    "        \n",
    "        # 找出最佳分割点\n",
    "        best_result = max(results, key=lambda x: x[0])\n",
    "        return best_result[0], int(best_result[1]), best_result[2], best_result[3]  \n",
    "        \n",
    "    def predict(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        return np.array([self._predict(x, self.root) for x in X])\n",
    "\n",
    "    def _predict(self, x, node):\n",
    "        if node.left is None and node.right is None:\n",
    "            return node.value\n",
    "        if np.isnan(x[node.feature_idx]):\n",
    "            return self._predict(x, node.left if node.default_left else node.right)\n",
    "        return self._predict(x, node.left if x[node.feature_idx] <= node.threshold else node.right)\n",
    "\n",
    "    def release_memory(self):\n",
    "        # 显式释放内存\n",
    "        del self.X\n",
    "        del self.column_blocks\n",
    "        del self.grad_stats \n",
    "        del self.quantile_sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8a7d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGBoost模型\n",
    "class XGBoostModel:\n",
    "    def __init__(self, params=None, random_seed=None, n_threads=1):\n",
    "        params = params or {}\n",
    "        self.params = params\n",
    "        self.base_value = params.get('base_value', 0.5)\n",
    "        self.subsample = params.get('subsample', 1.0)\n",
    "        self.learning_rate = params.get('learning_rate', 0.3)\n",
    "\n",
    "        self.n_threads = min(n_threads, os.cpu_count())\n",
    "        set_num_threads(self.n_threads)\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "        self.boosters = []\n",
    "        \n",
    "        self.avg_time_per_tree = None\n",
    "    \n",
    "    def fit(self, X, y, objective, rounds, verbose=False):\n",
    "        if isinstance(X, (csr_matrix, csc_matrix)):\n",
    "            X = X.toarray()\n",
    "        y = np.asarray(y)\n",
    "        n_samples = len(y)\n",
    "\n",
    "        preds = np.full(n_samples, self.base_value, dtype=np.float32)\n",
    "        grad_stats = np.empty((n_samples, 2), dtype=np.float32)\n",
    "        times = []\n",
    "\n",
    "        for i in range(rounds):\n",
    "            start_time = time.time()\n",
    "            grad_stats[:, 0] = objective.gradient(y, preds)\n",
    "            grad_stats[:, 1] = objective.hessian(y, preds)\n",
    "            \n",
    "            # 随机选择数据子集\n",
    "            if self.subsample < 1.0:\n",
    "                sample_size = math.floor(self.subsample * len(y))\n",
    "                indices = self.rng.choice(len(y), size=sample_size, replace=False)\n",
    "            else:\n",
    "                indices = None\n",
    "            \n",
    "            # 训练单个树\n",
    "            booster = BoosterTree(self.params, n_threads=self.n_threads)\n",
    "            booster.fit(X, grad_stats, indices)\n",
    "            \n",
    "            preds += self.learning_rate * booster.predict(X)\n",
    "            self.boosters.append(booster)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "            \n",
    "            # 释放内存\n",
    "            booster.release_memory()\n",
    "            \n",
    "            if verbose:\n",
    "                current_loss = objective.loss(y, preds)\n",
    "                print(f\"[{i}] Training Loss: {current_loss:.5f}\")\n",
    "        self.avg_time_per_tree = np.mean(times)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if isinstance(X, (csr_matrix, csc_matrix)):\n",
    "            X = X.toarray()\n",
    "        # 初始化预测值\n",
    "        predictions = np.full(len(X), self.base_value, dtype=np.float32)\n",
    "        \n",
    "        # 批量计算树的预测值\n",
    "        for booster in self.boosters:\n",
    "            predictions += self.learning_rate * booster.predict(X)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedcb02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SquaredErrorObjective:\n",
    "    def loss(self, y, pred):\n",
    "        return np.mean((y - pred)**2)\n",
    "    def gradient(self, y, pred):\n",
    "        return pred - y\n",
    "    def hessian(self, y, pred):\n",
    "        return np.ones(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185db4b-6bfe-46ab-a690-ac6e2f08f74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "def load_data(zip_name, file_name):\n",
    "    with ZipFile(zip_name) as zf:\n",
    "        with zf.open(file_name) as f:\n",
    "            df = pd.read_csv(f, na_values='?')\n",
    "\n",
    "    X = df.drop('Claim_Amount', axis=1) if file_name == 'train_set.csv' else df\n",
    "    y = (df['Claim_Amount'] > 0.0).astype(np.float32) if file_name == 'train_set.csv' else None\n",
    "\n",
    "    # 定义分类变量和连续变量列名\n",
    "    cat_cols = ['Calendar_Year','Model_Year', 'Blind_Make', 'Blind_Model', 'Blind_Submodel', \n",
    "                'Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5', 'Cat6', \n",
    "                'Cat7', 'Cat8', 'Cat9', 'Cat10', 'Cat11', 'Cat12', 'NVCat']\n",
    "    ord_cols = ['OrdCat']\n",
    "    cont_cols = ['Var1', 'Var2', 'Var3', 'Var4', 'Var5', 'Var6', 'Var7', 'Var8', \n",
    "                 'NVVar1', 'NVVar2', 'NVVar3', 'NVVar4']\n",
    "\n",
    "    # 处理分类变量的缺失值并转换为字符串\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"__MISSING__\").astype(str)\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from scipy.sparse import hstack\n",
    "\n",
    "    # 分类变量: 独热编码，使用-1和1编码\n",
    "    transformers = []\n",
    "    for col in cat_cols:\n",
    "        valid_categories = df[col].unique().reshape(-1, 1)\n",
    "\n",
    "        class CustomEncoder(OneHotEncoder):\n",
    "            def __init__(self, categories):\n",
    "                super().__init__(\n",
    "                    categories=categories,\n",
    "                    handle_unknown=\"ignore\",\n",
    "                    drop=\"first\",\n",
    "                    sparse_output=True\n",
    "                )\n",
    "\n",
    "            def transform(self, X):\n",
    "                # 获取普通的one-hot编码（稀疏矩阵格式）\n",
    "                encoded = super().transform(X)\n",
    "                # 将0转换为-1（保持稀疏格式）\n",
    "                encoded.data = np.where(encoded.data == 0, -1, encoded.data)\n",
    "                return encoded\n",
    "\n",
    "            def fit_transform(self, X, y=None):\n",
    "                self.fit(X)\n",
    "                return self.transform(X)\n",
    "\n",
    "        encoder = CustomEncoder(categories=[valid_categories.flatten()])\n",
    "        transformers.append((f\"encode_{col}\", encoder, [col]))\n",
    "\n",
    "    ct = ColumnTransformer(transformers, remainder=\"drop\")\n",
    "    X_cat_sparse = ct.fit_transform(df)\n",
    "    \n",
    "    # 连续变量: 均值填补\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_cont_filled = imputer.fit_transform(df[cont_cols])\n",
    "    \n",
    "    # 有序变量：替换'?'为0，并标准化\n",
    "    X_ord = X[ord_cols].fillna(0.)\n",
    "    scaler = StandardScaler()\n",
    "    ord_cat_scaled = scaler.fit_transform(X_ord)\n",
    "    X_ord_sparse = csr_matrix(X_ord)\n",
    "    \n",
    "    # 利用稀疏矩阵解决内存爆炸  \n",
    "    X_cont_sparse = csr_matrix(X_cont_filled)\n",
    "    X_processed_sparse = hstack([X_cat_sparse, X_ord_sparse, X_cont_sparse])\n",
    "\n",
    "    # 分割出10K数据，m=4225，差俩ID数据\n",
    "    if file_name == 'train_set.csv':\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_processed_sparse, y, \n",
    "                                                          train_size=10000, \n",
    "                                                          random_state=42)\n",
    "        return X_train, X_val, y_train, y_val\n",
    "    else:\n",
    "        return X_processed_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae35629-2f9e-4059-bbe7-579e3d1d9248",
   "metadata": {},
   "source": [
    "# 实验1：Allstate-10K对比稀疏算法有效性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ba4f4-e930-416a-bc8e-1a0a1bc5ec2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zip_name = './dataset/ClaimPredictionChallenge/train_set.zip'\n",
    "file_name = 'train_set.csv'\n",
    "X_train, X_val, y_train, y_val = load_data(zip_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c46fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Objective\n",
    "objective = SquaredErrorObjective()\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'max_depth': 5,\n",
    "    'reg_lambda': 1.0,\n",
    "    'gamma': 0.0,\n",
    "    'min_child_weight': 1.0,\n",
    "    'colsample_bynode': 0.6,\n",
    "    'subsample': 0.6,\n",
    "    'learning_rate':0.3,\n",
    "    'base_value':0.5,\n",
    "}\n",
    "rounds = 5\n",
    "\n",
    "# Experiment\n",
    "n_threads_list = [1, 2, 4, 8, 12]  # 根据硬件调整\n",
    "methods = [('approx', False), ('approx', True)]  # (method, sparse)\n",
    "results = {f\"{method}_{sparse}\": [] for method, sparse in methods}\n",
    "\n",
    "for method, sparse in methods:\n",
    "    for n_threads in n_threads_list:\n",
    "        params['method'] = method\n",
    "        params['sparse'] = sparse\n",
    "        model = XGBoostModel(params, random_seed=42, n_threads=n_threads)\n",
    "        model.fit(X_train, y_train, objective, rounds, verbose=True)\n",
    "        results[f\"{method}_{sparse}\"].append(model.avg_time_per_tree)\n",
    "        print(f'use threads: {model.n_threads}, \\\n",
    "            time cost: {model.avg_time_per_tree}, \\\n",
    "            method: {method}_{sparse} \\n')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "for method, sparse in methods:\n",
    "    label = f\"{method} {'sparse' if sparse else 'non-sparse'}\"\n",
    "    plt.plot(n_threads_list, results[f\"{method}_{sparse}\"], marker='o', label=label)\n",
    "plt.xlabel('Number of Threads')\n",
    "plt.ylabel('Time per Tree (sec)')\n",
    "plt.title('Impact of Sparsity-Aware Algorithm on Allstate-10K')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a10a0ce",
   "metadata": {},
   "source": [
    "# 验证损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估模型在验证集上的表现\n",
    "rounds = 10\n",
    "params['method'] = 'approx'\n",
    "params['sparse'] = True\n",
    "model = XGBoostModel(params, random_seed=42, n_threads=1)\n",
    "model.fit(X_train, y_train, objective, rounds, verbose=True)\n",
    "        \n",
    "# 在验证集上进行预测和评估\n",
    "if isinstance(X_val, csr_matrix):\n",
    "    X_val.data = X_val.data.astype(np.float32)\n",
    "else:\n",
    "    X_val = csr_matrix(X_val)\n",
    "    X_val.data = X_val.data.astype(np.float32)\n",
    "\n",
    "n_samples = min(10000, min(X_val.shape[0], len(y_val)))  # 确保不超过 y_val 的长度\n",
    "random_indices = np.random.choice(min(X_val.shape[0], len(y_val)), n_samples, replace=False)\n",
    "\n",
    "# 对稀疏矩阵和标签进行索引\n",
    "X_val_subset = X_val[random_indices, :]\n",
    "y_val_subset = np.array(y_val)[random_indices]\n",
    "\n",
    "def batch_predict_sparse(model, X, batch_size=1000):\n",
    "    n_samples = X.shape[0]\n",
    "    predictions = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch_X = X[i:end_idx, :]  # 确保使用二维索引\n",
    "        predictions[i:end_idx] = model.predict(batch_X)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 在子集上进行预测\n",
    "y_pred = batch_predict_sparse(model, X_val_subset, batch_size=1000)\n",
    "val_loss = objective.loss(y_val_subset, y_pred)\n",
    "print(f'use threads: {model.n_threads}, '\n",
    "      f'time cost: {model.avg_time_per_tree}, '\n",
    "      f'validation loss: {val_loss:.6f}, '\n",
    "      f'method: {method}_{sparse}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65627f67",
   "metadata": {},
   "source": [
    "# 标准XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc93b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# 标准 XGBoost 的参数设置\n",
    "xgb_params = {\n",
    "    'max_depth': params['max_depth'],\n",
    "    'lambda': params['reg_lambda'],\n",
    "    'gamma': params['gamma'],\n",
    "    'min_child_weight': params['min_child_weight'],\n",
    "    'colsample_bynode': params['colsample_bynode'],\n",
    "    'subsample': params['subsample'],\n",
    "    'eta': params['learning_rate'],\n",
    "    'tree_method': 'approx',  # 使用近似分裂算法\n",
    "    'nthread': n_threads,\n",
    "    'objective': 'reg:squarederror'\n",
    "}\n",
    "\n",
    "# 转换数据格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val_subset, label=y_val_subset)\n",
    "\n",
    "# 训练模型\n",
    "start_time = time.time()\n",
    "xgb_model = xgb.train(\n",
    "    xgb_params,\n",
    "    dtrain,\n",
    "    num_boost_round=rounds\n",
    ")\n",
    "xgb_time = (time.time() - start_time) / rounds\n",
    "\n",
    "# 预测和评估\n",
    "xgb_pred = xgb_model.predict(dval)\n",
    "xgb_loss = objective.loss(y_val_subset, xgb_pred)\n",
    "\n",
    "# 打印对比结果\n",
    "print(\"\\n=== 模型性能对比 ===\")\n",
    "print(f\"自实现 XGBoost:\")\n",
    "print(f\"- 训练时间 (每棵树): {model.avg_time_per_tree:.6f} 秒\")\n",
    "print(f\"- 验证集损失: {val_loss:.6f}\")\n",
    "print(f\"\\n标准 XGBoost:\")\n",
    "print(f\"- 训练时间 (每棵树): {xgb_time:.6f} 秒\")\n",
    "print(f\"- 验证集损失: {xgb_loss:.6f}\")\n",
    "print(f\"\\n性能提升:\")\n",
    "print(f\"- 时间提升: {(model.avg_time_per_tree/xgb_time - 1)*100:.2f}%\")\n",
    "print(f\"- 损失提升: {(val_loss/xgb_loss - 1)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a5cb5",
   "metadata": {},
   "source": [
    "# 问题记录\n",
    "1. 解决稀疏算法的用时更长的关键可能是：我们不需要存储所有特征，只储存非零特征，这点在column block的结构中要体现出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b93c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
